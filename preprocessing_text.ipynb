{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solar-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class PreprocessTweet:\n",
    "\n",
    "    def all_caps(self, tweet_word):\n",
    "        '''\n",
    "            Input is a string - tweet_word\n",
    "            Returns boolean whether the word has all capital letters\n",
    "        '''\n",
    "        isuppercase = re.match(r'^[A-Z]+$', tweet_word)\n",
    "        return bool(isuppercase)\n",
    "    \n",
    "    def repeating_letters(self, tweet_word):\n",
    "        '''\n",
    "            Input is a string - tweet_word\n",
    "            Return boolean whether the word has letter that repeats consequently\n",
    "        '''\n",
    "        for i in range(len(tweet_word)-2):\n",
    "            if tweet_word[i] == tweet_word[i+1] and tweet_word[i] == tweet_word[i+2]:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "\n",
    "    def tweet_patterns(self, tweet, multiple):\n",
    "        '''\n",
    "            Inputs:\n",
    "            - tweet of type string: it is the actual tweet\n",
    "            - multiple of type boolean: it represents whether to look for \n",
    "                multiple exclamation marks and \n",
    "                multiple words in capital letters and \n",
    "                multiple repeating letters in words\n",
    "            Returns three boolean variables in order:\n",
    "                has_exclamation_mark, has_all_caps, has_repeating_letters\n",
    "        '''\n",
    "        \n",
    "        has_exclamation_mark = False\n",
    "        has_all_caps = False\n",
    "        has_repeating_letters = False\n",
    "        \n",
    "        tweet_words = tweet.split()\n",
    "        list_all_caps = [self.all_caps(word) for word in tweet_words]\n",
    "        list_repeating_letters = [self.repeating_letters(word) for word in tweet_words]\n",
    "        \n",
    "        if multiple:\n",
    "            \n",
    "            counted_all_caps = Counter(list_all_caps)\n",
    "            counted_repeating_letters = Counter(list_repeating_letters)\n",
    "            counted_exclamation_marks = [Counter(word)['!'] for word in tweet_words]\n",
    "            \n",
    "            if counted_all_caps[True] > 1:\n",
    "                has_all_caps = True\n",
    "            if counted_repeating_letters[True] > 1:\n",
    "                has_repeating_letters = True\n",
    "            if sum(counted_exclamation_marks) > 1:\n",
    "                has_exclamation_mark = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if True in list_all_caps:\n",
    "                has_all_caps = True\n",
    "\n",
    "            if True in list_repeating_letters:\n",
    "                has_repeating_letters = True\n",
    "\n",
    "            if \"!\" in tweet:\n",
    "                has_exclamation_mark = True\n",
    "            \n",
    "        return has_exclamation_mark, has_all_caps, has_repeating_letters\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(self, tweet_df):\n",
    "        stop = stopwords.words('english')\n",
    "        tweet_df['stopwordsx'] = tweet_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "        return tweet_df\n",
    "    \n",
    "    \n",
    "    def get_wordnet_pos(self, tag):\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "        \n",
    "    def lemmatize(self, sentences):\n",
    "\n",
    "        tkn = TweetTokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        word_tokens = [tkn.tokenize(sentences[i]) for i in range(0, len(sentences))]\n",
    "        words_to_lemmatize = [nltk.pos_tag(word_tokens[i]) for i in range(0, len(sentences))]\n",
    "        \n",
    "        lemmatized = [[lemmatizer.lemmatize(word[0], self.get_wordnet_pos(word[1])) for word in sent] for sent in words_to_lemmatize]\n",
    "        \n",
    "        return lemmatized\n",
    "            \n",
    "    \n",
    "    def preprocess(self, tweet, use_stop, remove_numbers):\n",
    "        '''\n",
    "            Input is of type string: it is the actual tweet\n",
    "            Returns a tweet of type string which is:\n",
    "            - lowercased\n",
    "            - without any punctuation\n",
    "            - without user tags, i.e. words that start with @\n",
    "            - without multiple spaces between the words\n",
    "        '''\n",
    "        \n",
    "        tags = re.compile(r\"&.*;\")\n",
    "        if use_stop:\n",
    "            punct = re.compile(r'[{}]+'.format(re.escape(punctuation.replace(\"\\'\",\"\"))))\n",
    "        else:\n",
    "            punct = re.compile(r'[{}]+'.format(re.escape(punctuation)))\n",
    "        stopwords = re.compile(r'[{}]+'.format(re.escape(\"'\")))\n",
    "        non_ascii = re.compile('[^\\x00-\\x7F]+')\n",
    "        numbers = re.compile(r'[0-9]')\n",
    "        links = re.compile(r'http\\S+')\n",
    "        hashtags = re.compile(r'#\\w+')\n",
    "        user = re.compile(r\"@\\w+\")\n",
    "        spaces = re.compile(r\" +\")\n",
    "        \n",
    "        tweet = tweet.lower()\n",
    "        tweet = tags.sub('', tweet).strip()\n",
    "        tweet = user.sub('usertag', tweet).strip()\n",
    "        tweet = hashtags.sub('', tweet).strip()\n",
    "        tweet = links.sub('linktag', tweet).strip()\n",
    "        if remove_numbers:\n",
    "            tweet = numbers.sub('', tweet).strip()\n",
    "        tweet = non_ascii.sub('', tweet).strip()\n",
    "        tweet = punct.sub(' ', tweet).strip()\n",
    "        if use_stop:\n",
    "            tweet = stopwords.sub('', tweet)\n",
    "        tweet = spaces.sub(' ', tweet)\n",
    "        \n",
    "        return tweet\n",
    "    \n",
    "    def preprocess_tweets(self, tweets, pattern, use_stop, remove_numbers):\n",
    "        '''\n",
    "            Inputs:\n",
    "            - tweets of type DataFrame from pandas with 2 columns:\n",
    "                1. Sentiment Label\n",
    "                2. Tweet\n",
    "            - pattern of type boolean which is used for finding \n",
    "              single/multiple characters in the function tweet_patterns\n",
    "            Returns a new DataFrame from pandas with 5 columns:\n",
    "                1. Sentiment Label\n",
    "                2. Preprocessed Tweet\n",
    "                3. Whether the tweet had single/multiple exclamation marks\n",
    "                4. Whether the tweet had single/multiple words in capital letters\n",
    "                5. Whether the tweet had single/multiple words that had consequently repeating charaters \n",
    "        '''\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for raw_tweet in tweets.values:\n",
    "            record = []\n",
    "            mark, caps, letters = self.tweet_patterns(raw_tweet[1], pattern)\n",
    "            tweet = self.preprocess(raw_tweet[1], use_stop, remove_numbers)\n",
    "            \n",
    "            record.append(raw_tweet[0])\n",
    "            record.append(tweet)\n",
    "            record.append(mark)\n",
    "            record.append(caps)\n",
    "            record.append(letters)\n",
    "            data.append(record)\n",
    "            \n",
    "        df = pd.DataFrame(data, columns=['Label', 'Tweet', 'Mark', 'Caps', 'Letters'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-genealogy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
